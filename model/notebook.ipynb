{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        1,        20,         4, 887431883],\n",
       "       [        1,        33,         4, 878542699],\n",
       "       [        1,        61,         4, 878542420],\n",
       "       ...,\n",
       "       [      943,       570,         1, 888640125],\n",
       "       [      943,       808,         4, 888639868],\n",
       "       [      943,      1067,         2, 875501756]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.loadtxt('..\\\\data\\\\ml-100k\\\\ua.test', skiprows=0, delimiter='\\t').astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'ua.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'ua.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "\n",
    "    n_u = np.unique(total[:, 0]).size #num of users\n",
    "    n_i = np.unique(total[:, 1]).size #num of items\n",
    "\n",
    "    train_data = np.zeros((n_u, n_i), dtype='float32')\n",
    "    test_data = np.zeros((n_u, n_i), dtype='float32')\n",
    "\n",
    "    for i in range(train.shape[0]):\n",
    "        train_data[train[i][0]-1][train[i][1]-1] = train[i][2]\n",
    "    \n",
    "    for i in range(test.shape[0]):\n",
    "        test_data[test[i][0]-1][test[i][1]-1] = test[i][2]\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data(path='..\\\\data\\\\ml-100k\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3333333333333335"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1601\n",
    "sum(train_data[:, a])/np.count_nonzero(train_data[:, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_item_rating(train_data):\n",
    "    list_average_item_rating = [sum(train_data[:, i])/np.count_nonzero(train_data[:, i]) for i in range(train_data.shape[1])]\n",
    "    return list_average_item_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theba\\AppData\\Local\\Temp\\ipykernel_16580\\2928453520.py:2: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  list_average_item_rating = [sum(train_data[:, i])/np.count_nonzero(train_data[:, i]) for i in range(train_data.shape[1])]\n"
     ]
    }
   ],
   "source": [
    "list_average_item_rating = average_item_rating(train_data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5555555555555554"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_average_item_rating[1183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_features(item_path): \n",
    "   i_cols = ['movie id', 'movie title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n",
    " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "   movies = pd.read_csv(item_path, sep='|', names=i_cols,encoding='latin-1')\n",
    "   genres = movies[['unknown', 'Action', 'Adventure',\n",
    " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]\n",
    "   genres = genres.to_numpy()\n",
    "\n",
    "   return genres\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_featuers = get_item_features('..\\\\data\\\\ml-100k\\\\u.item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_featuers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, item_features: np.ndarray, labels: list) -> None:\n",
    "        self.item_features = item_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.item_features)\n",
    "    \n",
    "    def __getitem__(self, idx) -> None:\n",
    "        item = self.item_features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomDataset(item_features=item_featuers, labels=list_average_item_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = DataLoader(training_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df95f140fb95402880dca40a38ddfdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Theba\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Rating: 0.11542760580778122\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1) # 1 label for regression\n",
    "\n",
    "# Sample input description\n",
    "description = \"This product is amazing! It exceeded all my expectations.\"\n",
    "\n",
    "# Tokenize input description\n",
    "inputs = tokenizer(description, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predicted rating\n",
    "predicted_rating = outputs.logits.item()\n",
    "\n",
    "print(\"Predicted Rating:\", predicted_rating)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('google-bert/bert-base-uncased', num_labels=1) # 1 label for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch_community = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x1bf842016d0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(training_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify either input_ids or inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:972\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    970\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    974\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    975\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[1;31mValueError\u001b[0m: You have to specify either input_ids or inputs_embeds"
     ]
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3e7724b84a4cf187a254ae0b18e88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.6187, grad_fn=<MseLossBackward>), logits=tensor([[3.0470],\n",
      "        [3.0481],\n",
      "        [2.9785],\n",
      "        [3.0469],\n",
      "        [2.8587],\n",
      "        [3.2357],\n",
      "        [3.0540],\n",
      "        [2.9084]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0123, grad_fn=<MseLossBackward>), logits=tensor([[2.5740],\n",
      "        [2.2408],\n",
      "        [2.1672],\n",
      "        [2.1997],\n",
      "        [2.5548],\n",
      "        [2.5319],\n",
      "        [2.4621],\n",
      "        [2.3359]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3907, grad_fn=<MseLossBackward>), logits=tensor([[2.6733],\n",
      "        [2.9805],\n",
      "        [3.2224],\n",
      "        [2.8633],\n",
      "        [2.7398],\n",
      "        [2.9177],\n",
      "        [3.0406],\n",
      "        [2.8289]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8665, grad_fn=<MseLossBackward>), logits=tensor([[3.2827],\n",
      "        [3.2866],\n",
      "        [3.2950],\n",
      "        [3.2649],\n",
      "        [3.1751],\n",
      "        [3.1122],\n",
      "        [3.3156],\n",
      "        [3.2006]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3488, grad_fn=<MseLossBackward>), logits=tensor([[2.9490],\n",
      "        [3.0326],\n",
      "        [3.1401],\n",
      "        [2.9579],\n",
      "        [2.9982],\n",
      "        [2.9562],\n",
      "        [2.9699],\n",
      "        [3.0397]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.1759, grad_fn=<MseLossBackward>), logits=tensor([[2.8414],\n",
      "        [2.7092],\n",
      "        [2.9601],\n",
      "        [2.8148],\n",
      "        [2.9990],\n",
      "        [3.0266],\n",
      "        [2.9252],\n",
      "        [2.9048]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.5719, grad_fn=<MseLossBackward>), logits=tensor([[2.7643],\n",
      "        [2.6112],\n",
      "        [2.6336],\n",
      "        [2.8795],\n",
      "        [2.7499],\n",
      "        [2.9846],\n",
      "        [2.8432],\n",
      "        [2.5818]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.2972, grad_fn=<MseLossBackward>), logits=tensor([[2.9978],\n",
      "        [2.9234],\n",
      "        [2.5513],\n",
      "        [2.8092],\n",
      "        [2.9571],\n",
      "        [2.6856],\n",
      "        [2.7558],\n",
      "        [2.8551]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3353, grad_fn=<MseLossBackward>), logits=tensor([[3.1266],\n",
      "        [3.0403],\n",
      "        [3.1275],\n",
      "        [2.9875],\n",
      "        [2.9434],\n",
      "        [2.8016],\n",
      "        [3.2189],\n",
      "        [2.9778]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.6088, grad_fn=<MseLossBackward>), logits=tensor([[3.2110],\n",
      "        [3.2822],\n",
      "        [3.3178],\n",
      "        [3.1920],\n",
      "        [3.1527],\n",
      "        [3.0795],\n",
      "        [3.1302],\n",
      "        [3.2815]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.6715, grad_fn=<MseLossBackward>), logits=tensor([[3.3919],\n",
      "        [3.3142],\n",
      "        [3.3894],\n",
      "        [3.4190],\n",
      "        [3.4016],\n",
      "        [3.1451],\n",
      "        [3.3763],\n",
      "        [3.2518]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3408, grad_fn=<MseLossBackward>), logits=tensor([[3.3712],\n",
      "        [3.5249],\n",
      "        [3.3937],\n",
      "        [3.5182],\n",
      "        [3.4242],\n",
      "        [3.1409],\n",
      "        [3.1769],\n",
      "        [3.2299]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8645, grad_fn=<MseLossBackward>), logits=tensor([[3.3848],\n",
      "        [3.3148],\n",
      "        [3.2016],\n",
      "        [3.3903],\n",
      "        [3.1398],\n",
      "        [3.2241],\n",
      "        [3.6443],\n",
      "        [3.2674]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4464, grad_fn=<MseLossBackward>), logits=tensor([[3.0605],\n",
      "        [3.1709],\n",
      "        [3.1690],\n",
      "        [3.2313],\n",
      "        [3.2041],\n",
      "        [3.2560],\n",
      "        [3.2324],\n",
      "        [3.1062]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0548, grad_fn=<MseLossBackward>), logits=tensor([[3.0797],\n",
      "        [3.2549],\n",
      "        [3.0834],\n",
      "        [3.3526],\n",
      "        [3.0251],\n",
      "        [2.9023],\n",
      "        [2.7587],\n",
      "        [2.9146]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4261, grad_fn=<MseLossBackward>), logits=tensor([[2.7643],\n",
      "        [2.9310],\n",
      "        [2.8847],\n",
      "        [2.8547],\n",
      "        [2.9242],\n",
      "        [2.8087],\n",
      "        [2.8471],\n",
      "        [3.0191]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0879, grad_fn=<MseLossBackward>), logits=tensor([[2.7839],\n",
      "        [3.0762],\n",
      "        [2.5328],\n",
      "        [2.6971],\n",
      "        [2.6710],\n",
      "        [2.7369],\n",
      "        [2.9694],\n",
      "        [3.0242]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4650, grad_fn=<MseLossBackward>), logits=tensor([[2.9004],\n",
      "        [2.6320],\n",
      "        [2.8337],\n",
      "        [2.8853],\n",
      "        [2.8819],\n",
      "        [2.8294],\n",
      "        [2.7283],\n",
      "        [2.7948]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4312, grad_fn=<MseLossBackward>), logits=tensor([[2.9075],\n",
      "        [2.9156],\n",
      "        [2.7687],\n",
      "        [2.7734],\n",
      "        [2.7002],\n",
      "        [2.7056],\n",
      "        [2.5573],\n",
      "        [2.6384]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.6771, grad_fn=<MseLossBackward>), logits=tensor([[3.1101],\n",
      "        [2.5638],\n",
      "        [2.7850],\n",
      "        [3.1084],\n",
      "        [3.0318],\n",
      "        [2.7199],\n",
      "        [2.9584],\n",
      "        [2.6726]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.2677, grad_fn=<MseLossBackward>), logits=tensor([[2.9066],\n",
      "        [2.9579],\n",
      "        [2.9797],\n",
      "        [2.9917],\n",
      "        [2.9501],\n",
      "        [2.9646],\n",
      "        [2.8703],\n",
      "        [2.8653]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4449, grad_fn=<MseLossBackward>), logits=tensor([[2.8964],\n",
      "        [2.8810],\n",
      "        [2.9836],\n",
      "        [2.8951],\n",
      "        [3.0419],\n",
      "        [2.7428],\n",
      "        [2.9249],\n",
      "        [3.1195]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.1023, grad_fn=<MseLossBackward>), logits=tensor([[3.0838],\n",
      "        [3.1037],\n",
      "        [3.2160],\n",
      "        [3.0026],\n",
      "        [2.8802],\n",
      "        [2.9294],\n",
      "        [3.1218],\n",
      "        [3.0541]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.2578, grad_fn=<MseLossBackward>), logits=tensor([[2.9431],\n",
      "        [2.9816],\n",
      "        [3.1459],\n",
      "        [3.0451],\n",
      "        [2.9419],\n",
      "        [2.9710],\n",
      "        [3.0080],\n",
      "        [3.1124]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7745, grad_fn=<MseLossBackward>), logits=tensor([[2.9477],\n",
      "        [3.3018],\n",
      "        [2.7908],\n",
      "        [2.9024],\n",
      "        [3.2936],\n",
      "        [3.0542],\n",
      "        [3.1325],\n",
      "        [3.1525]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.5864, grad_fn=<MseLossBackward>), logits=tensor([[3.0530],\n",
      "        [3.2519],\n",
      "        [3.0699],\n",
      "        [3.2285],\n",
      "        [3.2550],\n",
      "        [3.2825],\n",
      "        [3.2262],\n",
      "        [3.3052]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.1544, grad_fn=<MseLossBackward>), logits=tensor([[3.1989],\n",
      "        [3.4645],\n",
      "        [3.2701],\n",
      "        [3.1453],\n",
      "        [3.2806],\n",
      "        [3.2349],\n",
      "        [3.3099],\n",
      "        [3.2070]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.3527, grad_fn=<MseLossBackward>), logits=tensor([[3.3255],\n",
      "        [3.0790],\n",
      "        [3.4244],\n",
      "        [3.1144],\n",
      "        [3.1939],\n",
      "        [3.2082],\n",
      "        [3.2625],\n",
      "        [3.4240]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.4072, grad_fn=<MseLossBackward>), logits=tensor([[3.1632],\n",
      "        [3.3749],\n",
      "        [3.4176],\n",
      "        [3.2438],\n",
      "        [3.0822],\n",
      "        [3.0961],\n",
      "        [3.3632],\n",
      "        [3.1838]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.6718, grad_fn=<MseLossBackward>), logits=tensor([[3.2058],\n",
      "        [3.2777],\n",
      "        [3.2239],\n",
      "        [3.2898],\n",
      "        [3.3094],\n",
      "        [3.2815],\n",
      "        [3.2290],\n",
      "        [2.9797]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8917, grad_fn=<MseLossBackward>), logits=tensor([[2.9611],\n",
      "        [3.1182],\n",
      "        [3.3666],\n",
      "        [3.0439],\n",
      "        [3.0833],\n",
      "        [3.1385],\n",
      "        [2.9001],\n",
      "        [3.1801]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0587, grad_fn=<MseLossBackward>), logits=tensor([[2.9842],\n",
      "        [3.2850],\n",
      "        [3.0121],\n",
      "        [3.1665],\n",
      "        [3.1469],\n",
      "        [2.9731],\n",
      "        [3.0485],\n",
      "        [3.0514]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9626, grad_fn=<MseLossBackward>), logits=tensor([[2.5894],\n",
      "        [3.1235],\n",
      "        [3.1412],\n",
      "        [2.8202],\n",
      "        [2.7903],\n",
      "        [2.9179],\n",
      "        [2.8379],\n",
      "        [2.8202]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.6199, grad_fn=<MseLossBackward>), logits=tensor([[2.9634],\n",
      "        [2.8784],\n",
      "        [2.6313],\n",
      "        [2.8758],\n",
      "        [2.6780],\n",
      "        [2.7210],\n",
      "        [2.6776],\n",
      "        [2.8306]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4543, grad_fn=<MseLossBackward>), logits=tensor([[2.5945],\n",
      "        [2.6253],\n",
      "        [2.6485],\n",
      "        [2.6485],\n",
      "        [2.7047],\n",
      "        [2.5895],\n",
      "        [2.6405],\n",
      "        [2.8665]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9267, grad_fn=<MseLossBackward>), logits=tensor([[2.5124],\n",
      "        [2.5423],\n",
      "        [2.7009],\n",
      "        [2.6557],\n",
      "        [2.6049],\n",
      "        [2.6891],\n",
      "        [2.5797],\n",
      "        [2.3949]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.6655, grad_fn=<MseLossBackward>), logits=tensor([[2.4586],\n",
      "        [2.5543],\n",
      "        [2.4595],\n",
      "        [2.6313],\n",
      "        [2.7251],\n",
      "        [2.3766],\n",
      "        [2.7575],\n",
      "        [2.5295]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7017, grad_fn=<MseLossBackward>), logits=tensor([[2.3780],\n",
      "        [2.2231],\n",
      "        [2.6681],\n",
      "        [2.7948],\n",
      "        [2.5218],\n",
      "        [2.6469],\n",
      "        [2.6814],\n",
      "        [2.6398]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0000, grad_fn=<MseLossBackward>), logits=tensor([[2.7040],\n",
      "        [2.4501],\n",
      "        [2.5507],\n",
      "        [2.5985],\n",
      "        [2.4576],\n",
      "        [2.9755],\n",
      "        [2.5280],\n",
      "        [2.6544]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4408, grad_fn=<MseLossBackward>), logits=tensor([[2.6012],\n",
      "        [2.6662],\n",
      "        [2.6480],\n",
      "        [2.7042],\n",
      "        [2.8348],\n",
      "        [2.4966],\n",
      "        [2.5527],\n",
      "        [2.4765]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4989, grad_fn=<MseLossBackward>), logits=tensor([[2.8166],\n",
      "        [2.5440],\n",
      "        [2.8048],\n",
      "        [3.0550],\n",
      "        [2.7814],\n",
      "        [2.7823],\n",
      "        [2.6960],\n",
      "        [2.6912]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.3044, grad_fn=<MseLossBackward>), logits=tensor([[2.5612],\n",
      "        [2.8337],\n",
      "        [2.9650],\n",
      "        [2.8115],\n",
      "        [2.9593],\n",
      "        [2.9921],\n",
      "        [2.7293],\n",
      "        [3.0931]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8290, grad_fn=<MseLossBackward>), logits=tensor([[2.7660],\n",
      "        [2.7023],\n",
      "        [2.8308],\n",
      "        [2.8952],\n",
      "        [2.8616],\n",
      "        [2.7864],\n",
      "        [3.0229],\n",
      "        [2.8739]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9925, grad_fn=<MseLossBackward>), logits=tensor([[2.7814],\n",
      "        [2.8863],\n",
      "        [2.9800],\n",
      "        [2.9929],\n",
      "        [2.9797],\n",
      "        [2.9754],\n",
      "        [2.8830],\n",
      "        [2.8359]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7152, grad_fn=<MseLossBackward>), logits=tensor([[2.8345],\n",
      "        [2.9534],\n",
      "        [2.9791],\n",
      "        [3.0410],\n",
      "        [2.9715],\n",
      "        [3.1927],\n",
      "        [3.1126],\n",
      "        [2.9218]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8712, grad_fn=<MseLossBackward>), logits=tensor([[2.9662],\n",
      "        [2.9370],\n",
      "        [2.8520],\n",
      "        [2.9878],\n",
      "        [3.0052],\n",
      "        [2.9432],\n",
      "        [2.9977],\n",
      "        [3.0662]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.6376, grad_fn=<MseLossBackward>), logits=tensor([[3.0670],\n",
      "        [3.0814],\n",
      "        [3.1530],\n",
      "        [2.9157],\n",
      "        [2.9368],\n",
      "        [2.9977],\n",
      "        [2.7247],\n",
      "        [3.0381]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.1180, grad_fn=<MseLossBackward>), logits=tensor([[3.1582],\n",
      "        [2.7232],\n",
      "        [3.0788],\n",
      "        [3.2409],\n",
      "        [3.0242],\n",
      "        [3.0441],\n",
      "        [2.9772],\n",
      "        [3.1156]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8788, grad_fn=<MseLossBackward>), logits=tensor([[3.2372],\n",
      "        [2.9844],\n",
      "        [2.9482],\n",
      "        [2.9104],\n",
      "        [3.1150],\n",
      "        [3.0990],\n",
      "        [3.0721],\n",
      "        [3.1384]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.4025, grad_fn=<MseLossBackward>), logits=tensor([[3.0929],\n",
      "        [3.2898],\n",
      "        [3.4491],\n",
      "        [3.1573],\n",
      "        [3.4232],\n",
      "        [3.2619],\n",
      "        [3.4021],\n",
      "        [3.1551]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.2564, grad_fn=<MseLossBackward>), logits=tensor([[3.1625],\n",
      "        [3.1017],\n",
      "        [3.2938],\n",
      "        [3.4119],\n",
      "        [3.1566],\n",
      "        [3.0259],\n",
      "        [3.2953],\n",
      "        [3.2202]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m((outputs))\n\u001b[0;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m----> 7\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Tensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    215\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    216\u001b[0m         relevant_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[0;32m    220\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\n\u001b[1;32m--> 221\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Theba\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 130\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in tqdm(range(num_epoch_community)):\n",
    "    for batch in training_data_loader:\n",
    "        outputs = model(input_ids = batch[0], labels=batch[1].float())\n",
    "        print((outputs))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list_average_item_rating[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]],\n",
       "        dtype=torch.int32),\n",
       " tensor([2.2941, 2.0000, 3.5455, 3.7692, 3.4615, 4.1556, 2.3529, 3.1264],\n",
       "        dtype=torch.float64)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(training_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfc30d227394634a3001df8e46c89e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996, 19081,\n",
      "          3075,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the Transformers library.\", return_tensors='pt')\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1837]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6690, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn((3, 5)).float()\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
